1 Installing and setting environment variables for Working with Apache
Hadoop 

2 Implementing Map-Reduce Program for Word Count problem,

3 Write a program to Implement a tri-gram model

4 Download and install Spark. Create Graphical data and access the
graphical data using Spark

5 Write a Spark code for the given application and handle error and
recovery of data

6 Write a Spark code to Handle the Streaming of data.

7 Install HBase and use the HBase Data model Store and retrieve data

8 Perform importing and exporting of data between SQL and Hadoop
using Sqoop.
Write a Pig Script for solving counting problems.

9 Use Flume and transport the data from the various sources to a
centralized data store


curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyBPkLCx0EGme8GRLP0dDuqyh4HGgVHqWVY" -H "Content-Type: application/json" -d "{\"contents\":[{\"parts\":[{\"text\":\"Explain how AI works in a few words\"}]}]}"
