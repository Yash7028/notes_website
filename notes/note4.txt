4.Write a Spark code to Handle the Streaming of data using RDD and
Data frame.

Installation Procedure:
Step 1: Go to Apache Spark's official download page and choose the latest release. For
the package type, choose ‘Pre-built for Apache Hadoop’.

Step 2: Once the download is completed, unzip the file, unzip the file using WinZip or
WinRAR, or 7-ZIP.

Step 3: Create a folder called Spark under your user Directory like below and copy and
paste the content from the unzipped file.
C:\Users\<USER>\Spark

Step 4: Go to the conf folder and open the log file called log4j.properties. template.
Change INFO to WARN (It can be an ERROR to reduce the log).

Step 5: Now, we need to configure the path.
Go to Control Panel -> System and Security -> System -> Advanced Settings ->
Environment Variables
Add below new user variable (or System variable) (To add a new user variable, click on
the New button under User variable for <USER>)

Click OK.
Add %SPARK_HOME%\bin to the path variable.

Click OK.

Step 6: Spark needs a piece of Hadoop to run. For Hadoop 3.3, you need to install
winutils.exe.
You can find winutils.exe on this link
https://github.com/steveloughran/winutils/tree/master You can download it for your
ease.

Step 7: Create a folder called winutils in C drive and create a folder called bin inside.
Then, move the downloaded winutils file to the bin folder.
C:\winutils\bin

Add the user (or system) variable %HADOOP_HOME% like SPARK_HOME.

Click OK.

Step 8: To install Apache Spark, Java should be installed on your computer. If you don’t
have java installed on your system. Please follow the below process

Java Installation Steps:
1. Go to the official Java site mentioned below the page.
Accept Licence Agreement for Java SE Development Kit 8u201
2. Download jdk-8u201-windows-x64.exe file
3. Double Click on the Downloaded .exe file
4. Click Next.
5. Then below window will be displayed.
6. Click Next.
7. Below window will be displayed after some process.
8. Click Close.

Test Java Installation
Open Command Line and type java -version, then it should display the installed version
of Java

You should also check JAVA_HOME and the path of %JAVA_HOME%\bin included in
user variables (or system variables)
1. In the end, the environment variables have 3 new paths (if you need to add a Java
path, otherwise SPARK_HOME and HADOOP_HOME).

Test Installation
Open the command line and type spark-shell, and you will get the result below.

We have completed the spark installation on the Windows system.

Creating and Accessing Graphical data in Spark:

Creating and accessing a bar graph in using pyspark:
To create a bar graph and access that bar graph in PySpark, you can use the Matplotlib
library to create the bar graph and then use Spark to work with the data that you want
to visualize in the bar graph. Here's a step-by-step guide:

1) Install Dependencies:
You'll need to install both PySpark and Matplotlib if you haven't already. You can use
pip for installation:
pip install pyspark
pip install matplotlib

2) Create a SparkSession:
Initialize a SparkSession to work with PySpark:

from pyspark.sql import SparkSession
spark=SparkSession.builder.appName("BarGraphExample").getOrCreate()

3) Prepare Data:
Prepare your data in a format that can be used to create a bar graph. For example, you
can read data from a CSV file into a DataFrame:
from pyspark.sql import Row

# Sample data
data = [Row(category="A", value=10),
Row(category="B", value=20),
Row(category="C", value=15),
Row(category="D", value=30)]

# Create a DataFrame
df = spark.createDataFrame(data)

4) Create the Bar Graph:
Use Matplotlib to create the bar graph based on the data in the DataFrame:
matplotlib.pyplot as plt

# Extract data from the DataFrame
categories = df.select("category").rdd.flatMap(lambda x: x).collect()
values = df.select("value").rdd.flatMap(lambda x: x).collect()

# Create the bar graph
plt.bar(categories, values)
plt.xlabel("Categories")
plt.ylabel("Values")
plt.title("Bar Graph Example")

# Display the graph
plt.show()

This code will create a simple bar graph using Matplotlib and allow you to access and
analyze your data using PySpark. You can customize the graph and data preparation
based on your specific requirements and dataset.

5) Access and Analyze Data:
You can still use Spark to perform operations on your data even after creating the bar
graph. For example, you can use Spark to aggregate, filter, or manipulate the data and
then update the bar graph accordingly.

Let's say you have a DataFrame with sales data for different product categories, and
you've already created a bar graph. Now, you want to update the bar graph to show the
total sales for each category and add a title to the graph. Here's an example:

from pyspark.sql importSparkSession
import matplotlib.pyplot as plt

# Create a SparkSession
Spark=SparkSession.builder.appName("BarGraphExample").getOrCreate()

# Sample data
data = [("Electronics", 1000),
("Clothing", 1500),
("Books", 800)
("Toys", 1200)]

# Create a DataFrame
columns = ["Category", "Sales"]
df = spark.createDataFrame(data, columns)

# Create the initial bar graph
categories = df.select("Category").rdd.flatMap(lambda x: x).collect()
sales = df.select("Sales").rdd.flatMap(lambda x: x).collect()
plt.bar(categories, sales)
plt.xlabel("Categories")
plt.ylabel("Sales")
plt.title("Bar Graph Example (Initial)")

# Display the initial graph
plt.show()

# Continue data analysis
# Calculate total sales per category
total_sales = df.groupBy("Category").sum("Sales").withColumnRenamed("sum(Sales)",
"TotalSales")

# Update the bar graph with total sales
updated_categories = total_sales.select("Category").rdd.flatMap(lambda x: x).collect()
updated_sales = total_sales.select("TotalSales").rdd.flatMap(lambda x: x).collect()

# Create an updated bar graph with total sales
plt.bar(updated_categories, updated_sales)
plt.xlabel("Categories")
plt.ylabel("Total Sales")
plt.title("Bar Graph Example (Updated)")

# Display the updated graph
plt.show()

6) Close Spark Session:
Don't forget to stop the Spark session when you're done:
spark.stop()







